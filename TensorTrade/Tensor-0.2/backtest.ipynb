{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pretrained model and test on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip tensortrade.zip\n",
    "#!unrar x LOB1_NQU22-CME_1min_2PercentSum_100PercentOrders_Overlapped_20Jun2022_19Sep2022.rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from link\n",
    "!wget -O data.zip https://www.dropbox.com/s/asg9ddvg46mr7iq/LOB1_NQZ22_1min_2PercentSum_100PercentOrders_Overlapped_2.zip?dl=0\n",
    "!unzip  '/content/data.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tabulate\n",
    "!pip install stable_baselines3\n",
    "!pip install gym\n",
    "!pip install deprecated\n",
    "!pip install stockstats\n",
    "!pip install zigzag\n",
    "!pip install unrar\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test and use gpu # pytorch\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Base Libs #####################\n",
    "#from tensortrade.data.inputs import *\n",
    "from tensortrade.data.feature_engineering_backtest import FeatureEngineering\n",
    "import tensortrade.env.default as default\n",
    "import tensortrade.env.env_stocktrading_train as Environment_Train\n",
    "import tensortrade.env.env_stocktrading_test as Environment_Test\n",
    "################### Others ##########################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config_Path= \"BackTest-configuration.json\"\n",
    "Config_File= open(Config_Path)\n",
    "Config = json.load(Config_File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see data path\n",
    "print('Data Path is :',Config['Data_Path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary feature engineering and data cleaning\n",
    "\n",
    "import shutil\n",
    "import ntpath\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "####################################\n",
    "\n",
    "def k_moving_average(arr, window_size=20):\n",
    "    moving_averages = []\n",
    "    i = 0\n",
    "    while i < len(arr) - window_size + 1:\n",
    "        window_average = round(np.sum(arr[i:i+window_size]) / window_size, 2)\n",
    "        moving_averages.append(window_average)\n",
    "        i += 1\n",
    "    return moving_averages\n",
    "    \n",
    "def process_data(path, ind, n, first_line, cols, dirname, level=10, k=[5,10,20,50,100], look_back=50):\n",
    "    data = [first_line]\n",
    "    with open(path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= ind[0] and i < ind[1]:\n",
    "                data.append(line)\n",
    "    ltps = []\n",
    "    for line in data[1:]:\n",
    "        splits = line.split('|')\n",
    "       \n",
    "        ltp = float(splits[cols['Candle_LastTradePrice']])\n",
    "        ltps.append(ltp)\n",
    "    multi_labels = []\n",
    "    for h in k:\n",
    "        ma = k_moving_average(ltps, h)\n",
    "        k_minus = np.array([np.nan] * (h-1) + ma)\n",
    "        k_plus = np.array(ma + [np.nan] * (h-1))\n",
    "        smoothing = (k_plus - k_minus) / k_minus\n",
    "        alpha = np.std(smoothing[~np.isnan(smoothing)]) / 2\n",
    "        labels = []\n",
    "        for s in smoothing[~np.isnan(smoothing)]:\n",
    "            if s > alpha:\n",
    "                labels.append(s)\n",
    "            elif s < -alpha:\n",
    "                labels.append(s)\n",
    "            else:\n",
    "                labels.append(s)\n",
    "        labels_str = [np.nan] * (h-1) + labels + [np.nan] * (h-1)\n",
    "        labels_str = [str(i) for i in labels_str]\n",
    "        multi_labels.append(labels_str)\n",
    "    data[0] = data[0][:-1] + '|Label1|Label2|Label3|Label4|Label5\\n'\n",
    "    for i in range(len(data)):\n",
    "        if i == 0:\n",
    "          continue\n",
    "        else:\n",
    "          splits = data[i].split('|')\n",
    "          bidprices = splits[cols['LOB_BidPrices']].split(',')\n",
    "          bidprices = [p for p in bidprices if p != ''][-level:]\n",
    "          askprices = splits[cols['LOB_AskPrices']].split(',')\n",
    "          askprices = [p for p in askprices if p != ''][:level]\n",
    "          bidvolumes = splits[cols['LOB_BidVolumes']].split(',')\n",
    "          bidvolumes = [v for v in bidvolumes if v != ''][-level:]\n",
    "          askvolumes = splits[cols['LOB_AskVolumes']].split(',')\n",
    "          askvolumes = [v for v in askvolumes if v != ''][:level]\n",
    "          splits[cols['LOB_BidPrices']] = ','.join(bidprices)\n",
    "          splits[cols['LOB_AskPrices']] = ','.join(askprices)\n",
    "          splits[cols['LOB_BidVolumes']] = ','.join(bidvolumes)\n",
    "          splits[cols['LOB_AskVolumes']] = ','.join(askvolumes)\n",
    "          splits[-1] = splits[-1][:-1]\n",
    "          splits.append(multi_labels[0][i-1])\n",
    "          splits.append(multi_labels[1][i-1])\n",
    "          splits.append(multi_labels[2][i-1])\n",
    "          splits.append(multi_labels[3][i-1])\n",
    "          splits.append(multi_labels[4][i-1] + '\\n')\n",
    "          data[i] = '|'.join(splits)\n",
    "    filename = ntpath.basename(path).split('.')[0] + 'labeled{}.csv'.format(n)\n",
    "    with open(os.path.join(dirname, 'labeled', filename), 'w') as f:\n",
    "        for line in data:\n",
    "            f.write(line)\n",
    "    corrupts = []\n",
    "    for i, line in enumerate(data):\n",
    "        splits = line.split('|')\n",
    "        bidprices = splits[cols['LOB_BidPrices']].split(',')\n",
    "        askprices = splits[cols['LOB_AskPrices']].split(',')\n",
    "        bidvolumes = splits[cols['LOB_BidVolumes']].split(',')\n",
    "        askvolumes = splits[cols['LOB_AskVolumes']].split(',')\n",
    "        if len(bidprices) < level or len(askprices) < level or '' in askprices or '' in bidprices:    # corrupt condition\n",
    "            corrupts.append(i)\n",
    "    print(len(corrupts))\n",
    "    subfiles = []\n",
    "    for i in range(len(corrupts) - 1):\n",
    "        start = corrupts[i]\n",
    "        end = corrupts[i+1]\n",
    "        if end - start - 1 >= look_back:\n",
    "            subfiles.append(data[start+1:end])\n",
    "    if (len(data) - 1 - corrupts[-1]) >= look_back:\n",
    "        subfiles.append(data[corrupts[-1]+1:])\n",
    "    total = []\n",
    "    for f in subfiles:\n",
    "        total += f\n",
    "    filename = ntpath.basename(path).split('.')[0] + '_clean_labeled_{}.csv'.format(n)\n",
    "    with open(os.path.join(dirname, 'clean_labeled', filename), 'w') as f:\n",
    "        for line in total:\n",
    "            f.write(line)\n",
    "    samplesX = []\n",
    "    samplesY = []\n",
    "    for f in subfiles:\n",
    "        for i in range(look_back-1, len(f)):\n",
    "            splits = f[i].split('|')\n",
    "            sample_labels = [float(splits[-5])+1, float(splits[-4])+1, float(splits[-3])+1, float(splits[-2])+1, float(splits[-1][:-1])+1]\n",
    "            if any([math.isnan(i) for i in sample_labels]):\n",
    "                continue\n",
    "            total_ask_prices = []\n",
    "            total_ask_volumes = []\n",
    "            total_bid_prices = []\n",
    "            total_bid_volumes = []\n",
    "            for j in range(i+1-look_back, i+1):\n",
    "                spltis = f[j].split('|')\n",
    "                total_ask_prices = total_ask_prices + splits[cols['LOB_AskPrices']].split(',')\n",
    "                total_bid_prices = total_bid_prices + splits[cols['LOB_BidPrices']].split(',')\n",
    "                if '' in total_bid_prices:\n",
    "                    print(splits[cols['LOB_BidPrices']])\n",
    "                    return splits[cols['LOB_BidPrices']]\n",
    "                total_ask_volumes = total_ask_volumes + splits[cols['LOB_AskVolumes']].split(',')\n",
    "                total_bid_volumes = total_bid_volumes + splits[cols['LOB_BidVolumes']].split(',')\n",
    "            total_ask_prices = np.array([float(i) for i in total_ask_prices])\n",
    "            total_ask_volumes = np.array([float(i) for i in total_ask_volumes])\n",
    "            total_bid_prices = np.array([float(i) for i in total_bid_prices])\n",
    "            total_bid_volumes = np.array([float(i) for i in total_bid_volumes])\n",
    "            price_mean = np.concatenate([total_ask_prices, total_bid_prices]).mean()\n",
    "            price_std = np.concatenate([total_ask_prices, total_bid_prices]).std()\n",
    "            volume_mean = np.concatenate([total_ask_volumes, total_bid_volumes]).mean()\n",
    "            volume_std = np.concatenate([total_ask_volumes, total_bid_volumes]).std()\n",
    "            total_ask_prices = (total_ask_prices - price_mean) / price_std\n",
    "            total_ask_volumes = (total_ask_volumes - volume_mean) / volume_std\n",
    "            total_bid_prices = (total_bid_prices - price_mean) / price_std\n",
    "            total_bid_volumes = (total_bid_volumes - volume_mean) / volume_std\n",
    "            total = np.stack([total_ask_prices, total_bid_prices, total_ask_volumes, total_bid_volumes]).T\n",
    "            total = total.reshape((look_back, -1))\n",
    "            samplesX.append(total)\n",
    "           \n",
    "            samplesY.append(sample_labels)\n",
    "    X_name = ntpath.basename(path).split('.')[0] + 'X{}.npy'.format(n)\n",
    "    y_name = ntpath.basename(path).split('.')[0] + 'y{}.npy'.format(n)\n",
    "    with open(os.path.join(dirname, 'numpy', X_name), 'wb') as f:\n",
    "        np.save(f, np.array(samplesX).reshape(-1, look_back, level*4, 1))\n",
    "    with open(os.path.join(dirname, 'numpy', y_name), 'wb') as f:\n",
    "        np.save(f, np.array(samplesY))\n",
    "\n",
    "def prepare_data_2(path, level=10, k=[5,10,20,50,100], look_back=50):\n",
    "    dirname = ntpath.basename(path).split('.')[0] + '_processed'\n",
    "    count = 0\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            if count == 0:\n",
    "                cols = line.split('|')\n",
    "                cols[-1] = cols[-1][:-1]\n",
    "                cols = {cols[i]:i for i in range(len(cols))}\n",
    "                first_line = line\n",
    "            count += 1\n",
    "   \n",
    "    n = (count // 21541) + 1\n",
    "    \n",
    "    inds = []\n",
    "    for i in range(n):\n",
    "        if (count - i*21541) > 21541:\n",
    "            if i == 0:\n",
    "                inds.append((1, (i+1)*21541))\n",
    "            else:\n",
    "                inds.append((i*21541, (i+1)*21541))\n",
    "        else:\n",
    "            inds.append((i*21541, count))\n",
    "    try:\n",
    "        shutil.rmtree(dirname)\n",
    "        print('Removing old directory and creating a new one')\n",
    "    except:\n",
    "        print('No such directory')\n",
    "        print('Creating new directory')\n",
    "        \n",
    "    os.mkdir(dirname)\n",
    "    os.mkdir(os.path.join(dirname, 'labeled'))\n",
    "    os.mkdir(os.path.join(dirname, 'clean_labeled'))\n",
    "    os.mkdir(os.path.join(dirname, 'numpy'))\n",
    "\n",
    "    for i, ind in enumerate(inds):\n",
    "        process_data(path=path, ind=ind, n=i, first_line=first_line, cols=cols, dirname=dirname, level=level, k=k, look_back=look_back)\n",
    "\n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_data_2(Config['Data_Resized_Path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather together cleaned datas\n",
    "D1=pd.read_csv('LOB1_NQZ22_1min_2PercentSum_100PercentOrders_Overlapped_2_processed/clean_labeled/LOB1_NQZ22_1min_2PercentSum_100PercentOrders_Overlapped_2_clean_labeled_0.csv',names= column_name, usecols = [i for i in range(33)], delimiter=\"|\")\n",
    "D2=pd.read_csv('LOB1_NQZ22_1min_2PercentSum_100PercentOrders_Overlapped_2_processed/clean_labeled/LOB1_NQZ22_1min_2PercentSum_100PercentOrders_Overlapped_2_clean_labeled_1.csv',names= column_name, usecols = [i for i in range(33)], delimiter=\"|\")\n",
    "D3=pd.read_csv('LOB1_NQZ22_1min_2PercentSum_100PercentOrders_Overlapped_2_processed/clean_labeled/LOB1_NQZ22_1min_2PercentSum_100PercentOrders_Overlapped_2_clean_labeled_2.csv',names= column_name, usecols = [i for i in range(33)], delimiter=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data= pd.concat([D1,D2,D3])\n",
    "Data[\"DateTime\"] = pd.to_datetime(Data[\"DateTime\"])\n",
    "Data.set_index([\"DateTime\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make neural net for RL model\n",
    "import gym\n",
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "#from torch import nn\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    :param observation_space: (gym.Space)\n",
    "    :param features_dim: (int) Number of features extracted.\n",
    "        This corresponds to the number of unit for the last layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int ):\n",
    "     \n",
    "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "        # We assume CxHxW images (channels first)\n",
    "        # Re-ordering will be done by pre-preprocessing or wrapper\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        # convolution blocks\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(1,4), stride=(1,2)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "#             nn.Tanh(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(1,4), stride=(1,2)),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(1,10)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "             nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(4,1)),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "        \n",
    "        # inception moduels\n",
    "        self.inp1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "        self.inp2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(5,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "        self.inp3 = nn.Sequential(\n",
    "            nn.MaxPool2d((3, 1), stride=(1, 1), padding=(1, 0)),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1,1), padding='same'),\n",
    "            nn.LeakyReLU(negative_slope=0.01),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "        # Compute shape by doing one forward pass\n",
    "        # with th.no_grad():\n",
    "        #     n_flatten = self.inp3(\n",
    "        #         th.as_tensor(observation_space.sample()[None]).float()\n",
    "        #     ).shape[1]\n",
    "        \n",
    "        # lstm layers\n",
    "        self.lstm = nn.LSTM(input_size=192, hidden_size=64, num_layers=1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(64, features_dim)\n",
    "\n",
    "    def forward(self, x: th.Tensor) -> th.Tensor:\n",
    "        \n",
    "        # return self.linear(self.lstm(self.cnn(observations)))\n",
    "        # h0: (number of hidden layers, batch size, hidden size)\n",
    "        h0 = torch.zeros(1, x.size(0), 64).to(device)\n",
    "        c0 = torch.zeros(1, x.size(0), 64).to(device)\n",
    "       \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        x_inp1 = self.inp1(x)\n",
    "        x_inp2 = self.inp2(x)\n",
    "        x_inp3 = self.inp3(x)  \n",
    "        \n",
    "        x = torch.cat((x_inp1, x_inp2, x_inp3), dim=1)\n",
    "        \n",
    "#         x = torch.transpose(x, 1, 2)\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        x = torch.reshape(x, (-1, x.shape[1], x.shape[2]))\n",
    "        x, _ = self.lstm(x, (h0, c0))\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc1(x)\n",
    "        # forecast_y = torch.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=49),\n",
    "    net_arch=[dict(vf=[64], pi=[64])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func to create environment\n",
    "\n",
    "def create_env_train(data,price,date,config):\n",
    "\n",
    "    env= Environment_Train.StockTradingEnv_Train(data,price,date,config)\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func to create test environment\n",
    "\n",
    "def create_env_test(data,price,date,config):\n",
    "\n",
    "    env= Environment_Test.StockTradingEnv_Test(data,price,date,config)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import RL agents\n",
    "from stable_baselines3 import A2C,DQN,PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL_model = A2C.load('RlSurf-A2C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Feature Engineering\n",
    "preProcessedData = FeatureEngineering(Data,Config['Feature_engineering'])\n",
    "Total_Train,Total_Train_unnormal,Total_dates = preProcessedData.add_all_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train And Test Based on WalkForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 10000\n",
    "train_size =test_size*2\n",
    "cursor=0\n",
    "######################\n",
    "while cursor <len(Total_Train):\n",
    "\n",
    "    if cursor+train_size >= len(Total_Train):\n",
    "\n",
    "        # make test data\n",
    "        Data_Test=Total_Train[cursor:]\n",
    "        Price_Data_Test = pd.DataFrame(Total_Train_unnormal[['open','high','low','close'][cursor:]]) # just price related columns\n",
    "        #Price_Data_Test.reset_index(drop=True,inplace=True)\n",
    "        Time_Data_Test = Total_dates[cursor:]  # just time column\n",
    "        # create test env\n",
    "        global env_test\n",
    "        env_test =  create_env_test(Data_Test.copy(),Price_Data_Test.copy(),Time_Data_Test,Config['env_config_test'])\n",
    "\n",
    "        #print(f'Train index {cursor} to {len(Total_Train)}')\n",
    "        # test RL model on test data\n",
    "        obs = env_test.get_state()    #~ we should not reset env manullay\n",
    "        number_candle=0 \n",
    "\n",
    "        while True: \n",
    "\n",
    "            action, _states = RL_model.predict(obs,deterministic=True)\n",
    "\n",
    "            number_candle+=1\n",
    "            obs, rewards, done, info = env_test.step(action)\n",
    "\n",
    "            if done:\n",
    "\n",
    "                if env_test.day_index>= len(env_test.day_indices)-1:\n",
    "                        print(\"Account Balance Is : \", info['account_status'])\n",
    "                        \n",
    "                        break\n",
    "                else:\n",
    "                    print('episode(Day) is : ',env_test.episode)\n",
    "                    env_test.reset()\n",
    "\n",
    "        break\n",
    "\n",
    "    else:\n",
    "\n",
    "        #### Train Part\n",
    "        s_idx = cursor\n",
    "        e_idx=cursor+train_size\n",
    "\n",
    "        # make train data\n",
    "        Time_Data = Total_Train[range(s_idx,e_idx)] # just time column\n",
    "        Data_Train=Total_dates[s_idx:e_idx]\n",
    "        Price_Data = pd.DataFrame(Total_Train_unnormal[['open','high','low','close'][range(s_idx,e_idx)]]) # just price related columns\n",
    "        # create train env\n",
    "        env_train= create_env_train(Data_Train.copy(),Price_Data.copy(),Time_Data,Config['env_config_train'])\n",
    "\n",
    "        # create RL model\n",
    "        #policy_kwargs = dict(net_arch=[8,16, dict(vf=[32,64,128,256], pi=[32,64,128,256])])\n",
    "        RL_model = A2C(Config['A2C_PARAMS']['net_arch'], env_train, verbose=1,device=device,\n",
    "        learning_rate=Config['A2C_PARAMS']['learning_rate'], policy_kwargs=policy_kwargs,seed=Config['A2C_PARAMS']['SEED'])\n",
    "        RL_model.learn(total_timesteps=Config['A2C_PARAMS']['TOTAL_TIMESTEPS'])\n",
    "   \n",
    "        #### Test Part\n",
    "        s_idx = cursor+train_size\n",
    "        e_idx= min(cursor+train_size+test_size,len(Total_Train))\n",
    "\n",
    "        # make test data\n",
    "        Data_Test=Total_Train[range(s_idx,e_idx)]\n",
    "        Price_Data_Test = pd.DataFrame(Total_Train_unnormal[['open','high','low','close'][range(s_idx,e_idx)]]) # just price related columns\n",
    "        #Price_Data_Test.reset_index(drop=True,inplace=True)\n",
    "        Time_Data_Test = Total_dates[s_idx:e_idx]  # just time column\n",
    "        # create test env\n",
    "        env_test =  create_env_test(Data_Test.copy(),Price_Data_Test.copy(),Time_Data_Test,Config['env_config_test'])\n",
    "\n",
    "        # test RL model on test data\n",
    "        obs = env_test.get_state()    #~ we should not reset env manullay\n",
    "        number_candle=0 \n",
    "\n",
    "        while True: \n",
    "\n",
    "            action, _states = RL_model.predict(obs,deterministic=True)\n",
    "\n",
    "            number_candle+=1\n",
    "            obs, rewards, done, info = env_test.step(action)\n",
    "\n",
    "            if done:\n",
    "\n",
    "                if env_test.day_index>= len(env_test.day_indices)-1:\n",
    "                        print(\"Account Balance Is : \", info['account_status'])\n",
    "                        \n",
    "                        break\n",
    "                else:\n",
    "                    print('episode(Day) is : ',env_test.episode)\n",
    "                    env_test.reset()\n",
    "        \n",
    "        #### update cursor\n",
    "        cursor+=test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('Ai2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e064f1da3ffc79377ac6706c76f9a3b70a86cfb41950670b0fa6e5938e2cfcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
