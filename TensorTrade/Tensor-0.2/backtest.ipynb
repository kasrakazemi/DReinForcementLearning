{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pretrained model and test on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip tensortrade.zip\n",
    "#!unrar x LOB1_NQU22-CME_1min_2PercentSum_100PercentOrders_Overlapped_20Jun2022_19Sep2022.rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from link\n",
    "!wget -O data.zip https://www.dropbox.com/s/asg9ddvg46mr7iq/LOB1_NQZ22_1min_2PercentSum_100PercentOrders_Overlapped_2.zip?dl=0\n",
    "!unzip  '/content/data.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tabulate\n",
    "!pip install stable_baselines3\n",
    "!pip install gym\n",
    "!pip install deprecated\n",
    "!pip install stockstats\n",
    "!pip install zigzag\n",
    "!pip install unrar\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test and use gpu # pytorch\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Base Libs #####################\n",
    "#from tensortrade.data.inputs import *\n",
    "from tensortrade.data.feature_engineering import FeatureEngineering\n",
    "import tensortrade.env.default as default\n",
    "import tensortrade.env.env_stocktrading_train as Environment_Train\n",
    "import tensortrade.env.env_stocktrading_test as Environment_Test\n",
    "################### Others ##########################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import optuna\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config_Path= \"configuration.json\"\n",
    "Config_File= open(Config_Path)\n",
    "Config = json.load(Config_File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see data path\n",
    "print('Data Path is :',Config['Data_Path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary feature engineering and data cleaning\n",
    "\n",
    "import shutil\n",
    "import ntpath\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "####################################\n",
    "\n",
    "def k_moving_average(arr, window_size=20):\n",
    "    moving_averages = []\n",
    "    i = 0\n",
    "    while i < len(arr) - window_size + 1:\n",
    "        window_average = round(np.sum(arr[i:i+window_size]) / window_size, 2)\n",
    "        moving_averages.append(window_average)\n",
    "        i += 1\n",
    "    return moving_averages\n",
    "    \n",
    "def process_data(path, ind, n, first_line, cols, dirname, level=10, k=[5,10,20,50,100], look_back=50):\n",
    "    data = [first_line]\n",
    "    with open(path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= ind[0] and i < ind[1]:\n",
    "                data.append(line)\n",
    "    ltps = []\n",
    "    for line in data[1:]:\n",
    "        splits = line.split('|')\n",
    "       \n",
    "        ltp = float(splits[cols['Candle_LastTradePrice']])\n",
    "        ltps.append(ltp)\n",
    "    multi_labels = []\n",
    "    for h in k:\n",
    "        ma = k_moving_average(ltps, h)\n",
    "        k_minus = np.array([np.nan] * (h-1) + ma)\n",
    "        k_plus = np.array(ma + [np.nan] * (h-1))\n",
    "        smoothing = (k_plus - k_minus) / k_minus\n",
    "        alpha = np.std(smoothing[~np.isnan(smoothing)]) / 2\n",
    "        labels = []\n",
    "        for s in smoothing[~np.isnan(smoothing)]:\n",
    "            if s > alpha:\n",
    "                labels.append(s)\n",
    "            elif s < -alpha:\n",
    "                labels.append(s)\n",
    "            else:\n",
    "                labels.append(s)\n",
    "        labels_str = [np.nan] * (h-1) + labels + [np.nan] * (h-1)\n",
    "        labels_str = [str(i) for i in labels_str]\n",
    "        multi_labels.append(labels_str)\n",
    "    data[0] = data[0][:-1] + '|Label1|Label2|Label3|Label4|Label5\\n'\n",
    "    for i in range(len(data)):\n",
    "        if i == 0:\n",
    "          continue\n",
    "        else:\n",
    "          splits = data[i].split('|')\n",
    "          bidprices = splits[cols['LOB_BidPrices']].split(',')\n",
    "          bidprices = [p for p in bidprices if p != ''][-level:]\n",
    "          askprices = splits[cols['LOB_AskPrices']].split(',')\n",
    "          askprices = [p for p in askprices if p != ''][:level]\n",
    "          bidvolumes = splits[cols['LOB_BidVolumes']].split(',')\n",
    "          bidvolumes = [v for v in bidvolumes if v != ''][-level:]\n",
    "          askvolumes = splits[cols['LOB_AskVolumes']].split(',')\n",
    "          askvolumes = [v for v in askvolumes if v != ''][:level]\n",
    "          splits[cols['LOB_BidPrices']] = ','.join(bidprices)\n",
    "          splits[cols['LOB_AskPrices']] = ','.join(askprices)\n",
    "          splits[cols['LOB_BidVolumes']] = ','.join(bidvolumes)\n",
    "          splits[cols['LOB_AskVolumes']] = ','.join(askvolumes)\n",
    "          splits[-1] = splits[-1][:-1]\n",
    "          splits.append(multi_labels[0][i-1])\n",
    "          splits.append(multi_labels[1][i-1])\n",
    "          splits.append(multi_labels[2][i-1])\n",
    "          splits.append(multi_labels[3][i-1])\n",
    "          splits.append(multi_labels[4][i-1] + '\\n')\n",
    "          data[i] = '|'.join(splits)\n",
    "    filename = ntpath.basename(path).split('.')[0] + 'labeled{}.csv'.format(n)\n",
    "    with open(os.path.join(dirname, 'labeled', filename), 'w') as f:\n",
    "        for line in data:\n",
    "            f.write(line)\n",
    "    corrupts = []\n",
    "    for i, line in enumerate(data):\n",
    "        splits = line.split('|')\n",
    "        bidprices = splits[cols['LOB_BidPrices']].split(',')\n",
    "        askprices = splits[cols['LOB_AskPrices']].split(',')\n",
    "        bidvolumes = splits[cols['LOB_BidVolumes']].split(',')\n",
    "        askvolumes = splits[cols['LOB_AskVolumes']].split(',')\n",
    "        if len(bidprices) < level or len(askprices) < level or '' in askprices or '' in bidprices:    # corrupt condition\n",
    "            corrupts.append(i)\n",
    "    print(len(corrupts))\n",
    "    subfiles = []\n",
    "    for i in range(len(corrupts) - 1):\n",
    "        start = corrupts[i]\n",
    "        end = corrupts[i+1]\n",
    "        if end - start - 1 >= look_back:\n",
    "            subfiles.append(data[start+1:end])\n",
    "    if (len(data) - 1 - corrupts[-1]) >= look_back:\n",
    "        subfiles.append(data[corrupts[-1]+1:])\n",
    "    total = []\n",
    "    for f in subfiles:\n",
    "        total += f\n",
    "    filename = ntpath.basename(path).split('.')[0] + '_clean_labeled_{}.csv'.format(n)\n",
    "    with open(os.path.join(dirname, 'clean_labeled', filename), 'w') as f:\n",
    "        for line in total:\n",
    "            f.write(line)\n",
    "    samplesX = []\n",
    "    samplesY = []\n",
    "    for f in subfiles:\n",
    "        for i in range(look_back-1, len(f)):\n",
    "            splits = f[i].split('|')\n",
    "            sample_labels = [float(splits[-5])+1, float(splits[-4])+1, float(splits[-3])+1, float(splits[-2])+1, float(splits[-1][:-1])+1]\n",
    "            if any([math.isnan(i) for i in sample_labels]):\n",
    "                continue\n",
    "            total_ask_prices = []\n",
    "            total_ask_volumes = []\n",
    "            total_bid_prices = []\n",
    "            total_bid_volumes = []\n",
    "            for j in range(i+1-look_back, i+1):\n",
    "                spltis = f[j].split('|')\n",
    "                total_ask_prices = total_ask_prices + splits[cols['LOB_AskPrices']].split(',')\n",
    "                total_bid_prices = total_bid_prices + splits[cols['LOB_BidPrices']].split(',')\n",
    "                if '' in total_bid_prices:\n",
    "                    print(splits[cols['LOB_BidPrices']])\n",
    "                    return splits[cols['LOB_BidPrices']]\n",
    "                total_ask_volumes = total_ask_volumes + splits[cols['LOB_AskVolumes']].split(',')\n",
    "                total_bid_volumes = total_bid_volumes + splits[cols['LOB_BidVolumes']].split(',')\n",
    "            total_ask_prices = np.array([float(i) for i in total_ask_prices])\n",
    "            total_ask_volumes = np.array([float(i) for i in total_ask_volumes])\n",
    "            total_bid_prices = np.array([float(i) for i in total_bid_prices])\n",
    "            total_bid_volumes = np.array([float(i) for i in total_bid_volumes])\n",
    "            price_mean = np.concatenate([total_ask_prices, total_bid_prices]).mean()\n",
    "            price_std = np.concatenate([total_ask_prices, total_bid_prices]).std()\n",
    "            volume_mean = np.concatenate([total_ask_volumes, total_bid_volumes]).mean()\n",
    "            volume_std = np.concatenate([total_ask_volumes, total_bid_volumes]).std()\n",
    "            total_ask_prices = (total_ask_prices - price_mean) / price_std\n",
    "            total_ask_volumes = (total_ask_volumes - volume_mean) / volume_std\n",
    "            total_bid_prices = (total_bid_prices - price_mean) / price_std\n",
    "            total_bid_volumes = (total_bid_volumes - volume_mean) / volume_std\n",
    "            total = np.stack([total_ask_prices, total_bid_prices, total_ask_volumes, total_bid_volumes]).T\n",
    "            total = total.reshape((look_back, -1))\n",
    "            samplesX.append(total)\n",
    "           \n",
    "            samplesY.append(sample_labels)\n",
    "    X_name = ntpath.basename(path).split('.')[0] + 'X{}.npy'.format(n)\n",
    "    y_name = ntpath.basename(path).split('.')[0] + 'y{}.npy'.format(n)\n",
    "    with open(os.path.join(dirname, 'numpy', X_name), 'wb') as f:\n",
    "        np.save(f, np.array(samplesX).reshape(-1, look_back, level*4, 1))\n",
    "    with open(os.path.join(dirname, 'numpy', y_name), 'wb') as f:\n",
    "        np.save(f, np.array(samplesY))\n",
    "\n",
    "def prepare_data_2(path, level=10, k=[5,10,20,50,100], look_back=50):\n",
    "    dirname = ntpath.basename(path).split('.')[0] + '_processed'\n",
    "    count = 0\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            if count == 0:\n",
    "                cols = line.split('|')\n",
    "                cols[-1] = cols[-1][:-1]\n",
    "                cols = {cols[i]:i for i in range(len(cols))}\n",
    "                first_line = line\n",
    "            count += 1\n",
    "   \n",
    "    n = (count // 21541) + 1\n",
    "    \n",
    "    inds = []\n",
    "    for i in range(n):\n",
    "        if (count - i*21541) > 21541:\n",
    "            if i == 0:\n",
    "                inds.append((1, (i+1)*21541))\n",
    "            else:\n",
    "                inds.append((i*21541, (i+1)*21541))\n",
    "        else:\n",
    "            inds.append((i*21541, count))\n",
    "    try:\n",
    "        shutil.rmtree(dirname)\n",
    "        print('Removing old directory and creating a new one')\n",
    "    except:\n",
    "        print('No such directory')\n",
    "        print('Creating new directory')\n",
    "        \n",
    "    os.mkdir(dirname)\n",
    "    os.mkdir(os.path.join(dirname, 'labeled'))\n",
    "    os.mkdir(os.path.join(dirname, 'clean_labeled'))\n",
    "    os.mkdir(os.path.join(dirname, 'numpy'))\n",
    "\n",
    "    for i, ind in enumerate(inds):\n",
    "        process_data(path=path, ind=ind, n=i, first_line=first_line, cols=cols, dirname=dirname, level=level, k=k, look_back=look_back)\n",
    "\n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_data_2(Config['Data_Resized_Path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather together cleaned datas\n",
    "D1=pd.read_csv('LOB1_NQZ22_1min_2PercentSum_100PercentOrders_Overlapped_2_processed/clean_labeled/LOB1_NQZ22_1min_2PercentSum_100PercentOrders_Overlapped_2_clean_labeled_0.csv',names= column_name, usecols = [i for i in range(33)], delimiter=\"|\")\n",
    "D2=pd.read_csv('LOB1_NQZ22_1min_2PercentSum_100PercentOrders_Overlapped_2_processed/clean_labeled/LOB1_NQZ22_1min_2PercentSum_100PercentOrders_Overlapped_2_clean_labeled_1.csv',names= column_name, usecols = [i for i in range(33)], delimiter=\"|\")\n",
    "D3=pd.read_csv('LOB1_NQZ22_1min_2PercentSum_100PercentOrders_Overlapped_2_processed/clean_labeled/LOB1_NQZ22_1min_2PercentSum_100PercentOrders_Overlapped_2_clean_labeled_2.csv',names= column_name, usecols = [i for i in range(33)], delimiter=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data= pd.concat([D1,D2,D3])\n",
    "Data[\"DateTime\"] = pd.to_datetime(Data[\"DateTime\"])\n",
    "Data.set_index([\"DateTime\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func to create test environment\n",
    "\n",
    "def create_env_test(data,price,date,config):\n",
    "\n",
    "    env= Environment_Test.StockTradingEnv_Test(data,price,date,config)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import RL agents\n",
    "from stable_baselines3 import A2C,DQN,PPO\n",
    "# import environment checker\n",
    "from stable_baselines3.common import env_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Feature Engineering\n",
    "preProcessedData = FeatureEngineering(Data,Config['Feature_engineering'])\n",
    "LOB_Data_test,LOB_Data_test_unnormal,LOB_Test_dates = preProcessedData.add_all_features()\n",
    "# make test data\n",
    "Data_Test=LOB_Data_test\n",
    "Price_Data_Test = pd.DataFrame(LOB_Data_test_unnormal[['open','high','low','close']]) # just price related columns\n",
    "#Price_Data_Test.reset_index(drop=True,inplace=True)\n",
    "Time_Data_Test = LOB_Test_dates  # just time column\n",
    "# create test env\n",
    "global env_test\n",
    "env_test =  create_env_test(Data_Test.copy(),Price_Data_Test.copy(),Time_Data_Test,Config['env_config_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RL_model = A2C.load('RlSurf-A2C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test RL model on test data\n",
    "obs = env_test.get_state()    #~ we should not reset env manullay\n",
    "number_candle=0 \n",
    "\n",
    "while True: \n",
    "\n",
    "    action, _states = RL_model.predict(obs,deterministic=True)\n",
    "\n",
    "    number_candle+=1\n",
    "    obs, rewards, done, info = env_test.step(action)\n",
    "\n",
    "    if done:\n",
    "\n",
    "        if env_test.day_index>= len(env_test.day_indices)-1:\n",
    "                print(\"Account Balance Is : \", info['account_status'])\n",
    "                \n",
    "                break\n",
    "        else:\n",
    "            print('episode(Day) is : ',env_test.episode)\n",
    "            env_test.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
